{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"EolfQ0LI_YuZ","executionInfo":{"status":"ok","timestamp":1605382991093,"user_tz":-120,"elapsed":2339,"user":{"displayName":"Idan Sheffer","photoUrl":"","userId":"03898896164623139330"}},"outputId":"3c4f95e4-1860-4f08-ce84-a74a65edc4c4","colab":{"base_uri":"https://localhost:8080/"}},"source":["import torch\n","from torch import nn\n","from tqdm import tqdm\n","from torchvision.datasets.mnist import FashionMNIST\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","\n","def create_datasets(data_root):\n","    trainset = FashionMNIST(root = data_root, train = True, download = True, transform = transforms.ToTensor())\n","    testset = FashionMNIST(root = data_root, train = False, download = True, transform = transforms.ToTensor())\n","    return trainset, testset\n","\n","\n","class ConvBlock(torch.nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n","        super().__init__()\n","        self.conv_block = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_block(x)\n","\n","\n","class PoolingBlock(torch.nn.Module):\n","    def __init__(self, kernel_size, stride):\n","        super().__init__()\n","        self.pooling_block = nn.AvgPool2d(kernel_size=kernel_size, stride=stride)\n","\n","    def forward(self, x):\n","        return self.pooling_block(x)\n","\n","class LeNet5(torch.nn.Module):\n","    def __init__(self, n_channels=1, batchnorm=False, dropout=False):\n","        super().__init__()\n","        self.n_channels = n_channels\n","        self.batchnorm = batchnorm\n","        self.do_dropout = dropout\n","\n","        self.conv1 = ConvBlock(in_channels=n_channels, out_channels=6, kernel_size=5, stride=1, padding=2)\n","        self.pooling1 = PoolingBlock(kernel_size=2, stride=2)\n","        self.conv2 = ConvBlock(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n","        self.pooling2 = PoolingBlock(kernel_size=2, stride=2)\n","        self.conv3 = ConvBlock(in_channels=16, out_channels=120, kernel_size=5, stride=1)\n","        self.activation = nn.Tanh()\n","        self.fc1 = nn.Linear(in_features=120, out_features=84)\n","        self.fc2 = nn.Linear(in_features=84, out_features=10)\n","\n","        if self.do_dropout:\n","            self.dropout = nn.Dropout(p=0.75)\n","\n","        if self.batchnorm:\n","            self.batch_norm1 = nn.BatchNorm2d(num_features=6)\n","            self.batch_norm2 = nn.BatchNorm2d(num_features=16)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.pooling1(x)\n","        if self.batchnorm:\n","            x = self.batch_norm1(x)\n","        x = self.conv2(x)\n","        x = self.pooling2(x)\n","        if self.batchnorm:\n","            x = self.batch_norm2(x)\n","        x = self.conv3(x)\n","        x = torch.flatten(x, start_dim=1)\n","        x = self.fc1(x)\n","        if self.do_dropout:\n","            x = self.dropout(x)\n","        x = self.activation(x)\n","        x = self.fc2(x)\n","        return x\n","\n","def compute_accuracy(est_labels, labels):\n","    correct_pred = torch.sum(est_labels == labels)\n","    total = labels.shape[0]\n","    return correct_pred / total\n","\n","def train_epoch(net, dataloader, loss_func, optimizer):\n","    net.train()\n","    all_losses = []\n","    all_acc = []\n","    for batch in dataloader:\n","        optimizer.zero_grad()\n","\n","        images, labels = batch\n","        images, labels = images.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n","        \n","        est_labels = net(images)\n","        loss = loss_func(est_labels, labels)\n","        all_losses.append(loss.item())\n","        \n","        if net.do_dropout:\n","            net.eval()\n","            all_acc.append(compute_accuracy(torch.argmax(net(images), dim=1), labels))\n","            net.train()\n","        else:\n","          all_acc.append(compute_accuracy(torch.argmax(est_labels, dim=1), labels))\n","\n","        loss.backward()\n","        optimizer.step()\n","    return all_losses, all_acc\n","\n","def eval(net, dataloader, loss_func):\n","    net.eval()\n","    all_losses = []\n","    all_acc = []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            images, labels = batch\n","            images, labels = images.to(device, dtype=torch.float), labels.to(device, dtype=torch.long)\n","        \n","            est_labels = net(images)\n","            all_losses.append(loss_func(est_labels, labels).item())\n","            all_acc.append(compute_accuracy(torch.argmax(est_labels, dim=1), labels))\n","\n","    return all_losses, all_acc\n","\n","def train(net, train_loader, val_loader, num_epochs, loss_func, optimizer):\n","    all_train_losses = []\n","    all_train_acc = []\n","    all_val_losses = []\n","    all_val_acc = []\n","    for epoch in range(num_epochs):\n","        print(\"Training Epoch %d / %d\" % (epoch, num_epochs))\n","        train_losses, train_acc = train_epoch(net, train_loader, loss_func, optimizer)\n","        all_train_losses.append(train_losses)\n","        all_train_acc.append(train_acc)\n","        val_losses, val_acc = eval(net, val_loader, loss_func)\n","        print(\"Epoch %d train loss: %f, train_acc: %f, val loss: %f, val accuracy: %f\" %\n","              (epoch, sum(train_losses) / len(train_losses), sum(train_acc) / len(train_acc),\n","               sum(val_losses) / len(val_losses), sum(val_acc) / len(val_acc)))\n","        all_val_losses.append(val_losses)\n","        all_val_acc.append(val_acc)\n","    print(\"Done training\")\n","    return all_train_losses, all_train_acc, all_val_losses, all_val_acc\n","\n","def define_regular_net():\n","  net = LeNet5().to(device)\n","  optimizer = torch.optim.Adam(net.parameters())\n","  return net, optimizer\n","\n","def define_dropout_net():\n","  net = LeNet5(dropout=True).to(device)\n","  optimizer = torch.optim.Adam(net.parameters())\n","  return net, optimizer\n","\n","def define_batchnorm_net():\n","  net = LeNet5(batchnorm=True).to(device)\n","  optimizer = torch.optim.Adam(net.parameters())\n","  return net, optimizer\n","\n","def define_weight_decay_net():\n","  net = LeNet5().to(device)\n","  optimizer = torch.optim.Adam(net.parameters(), weight_decay=1e-2)\n","  return net, optimizer\n","\n","def plot_graphs(train_losses, val_losses, train_acc, val_acc, net_str):\n","  plt.figure(figsize=(15, 15))\n","  plt.subplot(2, 2, 1)\n","  plt.plot(train_losses)\n","  plt.plot(val_losses)\n","  plt.legend(['train', 'val'])\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Loss\")\n","  plt.title(\"Training and Validation Loss\")\n","\n","  plt.subplot(2, 2, 2)\n","  plt.plot(train_acc)\n","  plt.plot(val_acc)\n","  plt.legend(['train', 'val'])\n","  plt.xlabel(\"Epoch\")\n","  plt.ylabel(\"Loss\")\n","  plt.title(f\"Accuracy - {net_str}\")\n","  plt.show()\n","\n","def choose_net_option(net_option):\n","  base_dir = \"/content/drive/Shared drives/Deep Learning Course/ex1_204094213_301861902/net_weights\"\n","  if net_option == 0:\n","    net ,optimizer = define_regular_net()\n","    weights_path = f\"{base_dir}/LeNet5_NoRegularization.pt\"\n","    net_str = \"No Regularization\"\n","  if net_option == 1:\n","    net ,optimizer = define_batchnorm_net()\n","    weights_path = f\"{base_dir}/LeNet5_BatchNorm.pt\"\n","    net_str = \"BatchNorm Conv. layers\"\n","  if net_option == 2:\n","    net ,optimizer = define_weight_decay_net()\n","    weights_path = f\"{base_dir}/LeNet5_WeightDecay.pt\"\n","    net_str = \"Weight Decay\"\n","  if net_option == 3:\n","    net ,optimizer = define_dropout_net()\n","    weights_path = f\"{base_dir}/LeNet5_Dropout.pt\"\n","    net_str = \"Dropout FC1 (p=0.75)\"\n","  return net, optimizer, weights_path, net_str\n","\n","def train_net(net_option):\n","  net, optimizer, weights_path, net_str = choose_net_option(net_option)\n","  \n","  loss_func = torch.nn.CrossEntropyLoss()\n","  num_epochs = 20\n","  all_train_losses, all_train_acc, all_val_losses, all_val_acc = train(net, train_loader, test_loader, num_epochs, loss_func, optimizer)\n","  # torch.save(net.state_dict(), weights_path)\n","\n","  train_losses = [sum(x) / len(x) for x in all_train_losses]\n","  train_acc = [sum(x) / len(x) for x in all_train_acc]\n","  val_losses = [sum(x) / len(x) for x in all_val_losses]\n","  val_acc = [sum(x) / len(x) for x in all_val_acc]\n","\n","  print(\"Latest Accuracy train:\", train_acc[-1].item())\n","  print(\"Latest Accuracy val:\", val_acc[-1].item())\n","\n","  plot_graphs(train_losses, val_losses, train_acc, val_acc, net_str)\n","\n","def test_net(net_option, test_loader):\n","  net, optimizer, weights_path, _ = choose_net_option(net_option)\n","\n","  saved_weights = torch.load(weights_path)\n","  net.load_state_dict(saved_weights)\n","  \n","  loss_func = torch.nn.CrossEntropyLoss()\n","  all_losses, all_acc = eval(net, test_loader, loss_func)\n","\n","  val_losses = sum(all_losses) / len(all_losses)\n","  val_acc = sum(all_acc) / len(all_acc)\n","\n","  print(\"Accuracy val:\", val_acc.item())\n","\n","class NetOption:\n","  regular = 0\n","  batchnorm = 1\n","  weight_decay = 2\n","  dropout = 3\n","\n","drive.mount('/content/drive')\n","trainset, testset = create_datasets(\"./data\")\n","batch_size = 128\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size, num_workers=4)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","net_option = NetOption.regular\n","# train_net(net_option)\n","test_net(net_option, test_loader)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Accuracy val: 0.895866334438324\n"],"name":"stdout"}]}]}