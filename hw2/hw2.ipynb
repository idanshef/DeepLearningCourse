{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0n0Goz0MO92e",
        "outputId": "fef8598f-3f32-4b8f-c191-7bebef4f2559"
      },
      "source": [
        "!pip install pytorch-nlp\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy\n",
        "import sys\n",
        "from torchnlp.datasets import penn_treebank_dataset\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "class WordDict:\n",
        "  def __init__(self):\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = []\n",
        "  \n",
        "  def add_word(self, word):\n",
        "    if word not in self.word2idx:\n",
        "      self.idx2word.append(word)\n",
        "      self.word2idx[word] = len(self.idx2word) - 1\n",
        "    return self.word2idx[word]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.idx2word)\n",
        " \n",
        "\n",
        "class Corpus:\n",
        "  def __init__(self, train_set, val_set, test_set):\n",
        "    self.word_dict = WordDict()\n",
        "    self.train = self.tokenize(train_set)\n",
        "    self.val = self.tokenize(val_set)\n",
        "    self.test = self.tokenize(test_set)\n",
        "\n",
        "  def tokenize(self, data_set):\n",
        "    tokens = len(data_set)\n",
        "    for word in data_set:\n",
        "      self.word_dict.add_word(word)\n",
        "    \n",
        "    ids = torch.LongTensor(tokens)\n",
        "    for token, word in enumerate(data_set):\n",
        "      ids[token] = self.word_dict.word2idx[word]\n",
        "    \n",
        "    return ids\n",
        "\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    if torch.cuda.is_available():\n",
        "        data = data.cuda()\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_batch(source, i):\n",
        "  seq_len = min(sequnce_length, len(source) - 1 - i)\n",
        "  data = Variable(source[i:i+seq_len])\n",
        "  target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
        "  return data, target\n",
        "\n",
        "\n",
        "class RnnModel(nn.Module):\n",
        "  def __init__(self, rnn_type, num_tokens, num_input=200, num_hidden=200, num_layers=2, dropout_p=0.5):\n",
        "    super(RnnModel, self).__init__()\n",
        "    assert rnn_type in ['LSTM', 'GRU'], \"rnn_type must be one of ['LSTM', GRU'], got %s\" % rnn_type\n",
        "    self.dropout_p = dropout_p\n",
        "    self.rnn_type = rnn_type\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hidden = num_hidden\n",
        "    \n",
        "    self.embedding = nn.Embedding(num_tokens, num_input)\n",
        "    self.rnn = getattr(nn, rnn_type)(num_input, num_hidden, num_layers, dropout=dropout_p)\n",
        "    self.dropout = nn.Dropout(dropout_p)\n",
        "    # maybe tie weight of emmbedding and hidden2token?\n",
        "    self.hidden2token = nn.Linear(num_hidden, num_tokens)\n",
        "    assert num_input == num_hidden, \"For weight tieing embedding size must be same as hidden size\"\n",
        "    self.hidden2token.weight = self.embedding.weight\n",
        "    \n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "    init_range = 0.1\n",
        "    self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "    self.hidden2token.bias.data.fill_(0.)\n",
        "    self.hidden2token.weight.data.uniform_(-init_range, init_range)\n",
        "  \n",
        "  def init_hidden(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    if self.rnn_type == 'LSTM':\n",
        "        return (Variable(weight.new(self.num_layers, batch_size, self.num_hidden).zero_()).to(weight.device),\n",
        "                Variable(weight.new(self.num_layers, batch_size, self.num_hidden).zero_()).to(weight.device))\n",
        "    else:\n",
        "        return Variable(weight.new(self.num_layers, batch_size, self.num_hidden).zero_())\n",
        "\n",
        "  def forward(self, input, hidden):\n",
        "    embedding = self.dropout(self.embedding(input))\n",
        "    out, hidden = self.rnn(embedding, hidden)\n",
        "    out = self.dropout(out)\n",
        "    tokens = self.hidden2token(out.view(out.shape[0] * out.shape[1], out.shape[2]))\n",
        "    return tokens.view(out.shape[0], out.shape[1], tokens.shape[1]), hidden \n",
        "    \n",
        "def repackage_hidden(h):\n",
        "  \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
        "  if type(h) == tuple:\n",
        "      return tuple(Variable(v) for v in h)\n",
        "  else:\n",
        "      return Variable(h.data)\n",
        "\n",
        "def train_epoch(rnn_model, train_data, loss_fn, optimizer):\n",
        "  rnn_model.train()\n",
        "  total_loss = 0\n",
        "  num_tokens = len(corpus.word_dict)\n",
        "  hidden = rnn_model.init_hidden(batch_size)\n",
        "  for i in range(0, train_data.shape[0] - 1, sequnce_length):\n",
        "    optimizer.zero_grad()\n",
        "    data, targets = get_batch(train_data, i)\n",
        "    hidden = repackage_hidden(hidden)\n",
        "    output, hidden = rnn_model(data, hidden)\n",
        "    loss = loss_fn(output.view(-1, num_tokens), targets)\n",
        "    loss.backward()\n",
        "    # torch.nn.utils.clip_grad_norm_(rnn_model.parameters(), 1.)\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += len(data) * loss.item()\n",
        "  return rnn_model, total_loss / len(train_data)\n",
        "\n",
        "\n",
        "def evaluate(rnn_model, eval_data, loss_fn):\n",
        "  rnn_model.eval()\n",
        "  with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    num_tokens = len(corpus.word_dict)\n",
        "    hidden = rnn_model.init_hidden(batch_size)\n",
        "    for i in range(0, eval_data.shape[0] - 1, sequnce_length):\n",
        "      data, targets = get_batch(eval_data, i)\n",
        "      output, hidden = rnn_model(data, hidden)\n",
        "      output_flat = output.view(-1, num_tokens)\n",
        "      total_loss += len(data) * loss_fn(output_flat, targets).item()\n",
        "      hidden = repackage_hidden(hidden)\n",
        "  return total_loss / len(eval_data)\n",
        "\n",
        "def save_model(path, model):\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "def train(rnn_model, train_data, val_data, loss_fn, optimizer, num_epochs, lr_scheduler=None):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  best_val_loss = sys.float_info.max\n",
        "  best_model = None\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    rnn_model, train_loss = train_epoch(rnn_model, train_data, loss_fn, optimizer)\n",
        "    train_losses.append(train_loss)\n",
        "    val_loss = evaluate(rnn_model, val_data, loss_fn)\n",
        "    val_losses.append(val_loss)\n",
        "    if lr_scheduler is not None:\n",
        "      lr_scheduler.step()\n",
        "    print(\"Epoch %d:\\n\\t train loss %f, train preplexity %f\\n\\t validation loss %f, validation preplexity %f\" % (epoch, train_loss, numpy.exp(train_loss), val_loss, numpy.exp(val_loss)))\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      best_model = rnn_model\n",
        "  \n",
        "  return best_model, train_losses, val_losses\n",
        "\n",
        "\n",
        "def plot_graphs(train_preplexity, val_preplexity, lr, keep_prob, type_name):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "  plt.subplot(2, 2, 1)\n",
        "  plt.plot(train_preplexity)\n",
        "  plt.plot(val_preplexity)\n",
        "  plt.legend(['train', 'val'])\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Preplexity\")\n",
        "  plt.title(\"Preplexity - Type: %s, LR: %.2e, Keep Prob.: %.2f\" % (type_name, lr, keep_prob))\n",
        "\n",
        "\n",
        "def train_net(type_name, dropout, num_tokens, train_data, val_data, loss_fn):\n",
        "  rnn_model = RnnModel(type_name, num_tokens, dropout_p=dropout)\n",
        "  if torch.cuda.is_available():\n",
        "    rnn_model.cuda()\n",
        "\n",
        "  num_epochs = 25\n",
        "  lr = 1e-3\n",
        "  optimizer = torch.optim.Adam(rnn_model.parameters(), lr=lr)\n",
        "  lr_scheduler = None # torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "  best_model, train_losses, val_losses = train(rnn_model, train_data, val_data, loss_fn, optimizer, num_epochs, lr_scheduler)\n",
        "\n",
        "  train_preplexity = [numpy.exp(x) for x in train_losses]\n",
        "  val_preplexity = [numpy.exp(x) for x in val_losses]\n",
        "  plot_graphs(train_preplexity, val_preplexity, lr=lr, keep_prob=1. - dropout, type_name=type_name)\n",
        "\n",
        "  return best_model\n",
        "\n",
        "\n",
        "class UserOption:\n",
        "  train = 0\n",
        "  LSTM = 1\n",
        "  LSTM_DROPOUT = 2\n",
        "  GRU = 3\n",
        "  GRU_DROPOUT = 4\n",
        "\n",
        "\n",
        "def load_net(net_type, loss_fn, num_tokens):\n",
        "  net_dir = \"/content/drive/Shared drives/Deep Learning Course/ex2_204094213_301861902/net_weights\"\n",
        "  if net_type == UserOption.LSTM:\n",
        "    rnn_model = RnnModel('LSTM', num_tokens, dropout_p=0)\n",
        "    weights_path = f\"{net_dir}/LSTM.pt\"\n",
        "  elif net_type == UserOption.LSTM_DROPOUT:\n",
        "    rnn_model = RnnModel('LSTM', num_tokens, dropout_p=0.5)\n",
        "    weights_path = f\"{net_dir}/LSTM_Dropout.pt\"\n",
        "  elif net_type == UserOption.GRU:\n",
        "    rnn_model = RnnModel('GRU', num_tokens, dropout_p=0)\n",
        "    weights_path = f\"{net_dir}/GRU.pt\"\n",
        "  elif net_type == UserOption.GRU_DROPOUT:\n",
        "    rnn_model = RnnModel('GRU', num_tokens, dropout_p=0.5)\n",
        "    weights_path = f\"{net_dir}/GRU_Dropout.pt\"\n",
        "  \n",
        "  if torch.cuda.is_available():\n",
        "    rnn_model.cuda()\n",
        "\n",
        "  saved_weights = torch.load(weights_path)\n",
        "  rnn_model.load_state_dict(saved_weights)\n",
        "  return rnn_model\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"/content/drive/Shared drives/Deep Learning Course/ex2_204094213_301861902/data\"\n",
        "train_set, val_set, test_set = penn_treebank_dataset(data_dir, train=True, dev=True, test=True)\n",
        "corpus = Corpus(train_set, val_set, test_set)\n",
        "\n",
        "batch_size = 32\n",
        "sequnce_length = 35\n",
        "train_data = batchify(corpus.train, batch_size)\n",
        "val_data = batchify(corpus.val, batch_size)\n",
        "test_data = batchify(corpus.test, batch_size)\n",
        "\n",
        "num_tokens = len(corpus.word_dict)\n",
        "\n",
        "########## USER CONTROL ##########\n",
        "user_option = UserOption.GRU_DROPOUT\n",
        "dropout = 0.5 # change to 0 for no dropout\n",
        "type_name = 'GRU' # valid options are 'LSTM' / 'GRU'\n",
        "########## USER CONTROL ##########\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "if user_option == UserOption.train:\n",
        "  net_model = train_net(type_name, dropout, num_tokens, train_data, val_data, loss_fn)\n",
        "  save_model(\"/content/drive/Shared drives/Deep Learning Course/ex2_204094213_301861902/net_weights/GRU_Dropout.pt\", net_model)\n",
        "else:\n",
        "  net_model = load_net(user_option, loss_fn, num_tokens)\n",
        "\n",
        "test_perplexity = numpy.exp(evaluate(net_model, test_data, loss_fn))\n",
        "print(f\"test_perplexity = {test_perplexity}\")\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-nlp) (1.18.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "test_perplexity = 106.56057677485713\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}